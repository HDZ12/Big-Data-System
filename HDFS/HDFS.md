# 2.1简介
- 集群：集群就是逻辑上处理同一任务的机器集合，可以属于同一机房，也可分属不同的机房
- 分布式文件系统：把文件分布存储到多个计算机节点上，成千上万的计算机节点构成计算机集群
- 与之前使用多个处理器和专用高级硬件的并行化处理装置不同的是，目前的分布式文件系统所采用的计算机集群，都是由普通硬件构成的，这就大大降低了硬件上的开销
![15b4cd6e9ca28cdceb48249a9f5a59ce](https://github.com/HDZ12/Big-Data-System/assets/99587726/a8b9dba1-27e5-454e-8794-723c3f840763)
## 2.1.2分布式文件系统结构
分布式文件系统在物理结构上是由计算机集群中的多个节点构成的，这些节点分为两类，一类叫“主节点”(Master Node)或者也被称为“名称结点”(NameNode)，另一类叫“从节点”（Slave Node）或者也被称为“数据节点”(DataNode)
![08e53a6067a18f26629b72f61ce14a8a](https://github.com/HDZ12/Big-Data-System/assets/99587726/9cc5a0a5-81c1-44c4-90f1-95cf9283759f)
### 设计需求

分布式文件系统的设计目标主要包括透明性、并发控制、可伸缩性、容错以及安全需求等。
|设计需求|含义|HDFS实现情况|
|--|--|--|
|透明性|具备访问透明性、位置透明性、性能和伸缩透明性。访问透明性是指用户不需要专门区分哪些是本地文件，哪些是远程文件，用户能够通过相同的操作来访问本地和远程文件资源。位置透明性是指在不改变路径名的前提下，不管文件副本数量和实际存储位置发生何种变化，对用户而言都是透明的，用户不会感受到这种变化，只需要使用相同的路径名就始终可以访问同一个文件。性能和伸缩透明性是指系统中节点的增加或减少以及性能的变化对用户而言是透明的，用户感受不到什么时候一个节点加入或退出了。|只能提供一定程度的访问透明性，完全支持位置透明性、性能和伸缩透明性。|
|并发控制|客户端对于文件的读写不应该影响其他客户端对同一个文件的读写|机制很简单，任何时间都只允许有一个程序写入某个文件|
|文件复制|一个文件可以拥有在不同位置的多个副本|HDFS采用了多个副本机制|
|硬件和操作系统的异构性|可以在不同的操作系统和计算机上实现同样的客户端和服务器端程序|采用Java语言开发，具有很好的跨平台能力|
|可伸缩性|支持节点的动态加入或退出|建立在大规模廉价机器上的分布式文件系统集群，具有很好的可伸缩性|
|容错|保证文件服务在客户端或者服务器端出现问题的时候能正常使用|具有多副本机制和故障自动检测、恢复机制|
|安全|保障系统的安全性|安全性较弱|
## 2.1.3HDFS特性

- 兼容廉价的硬件设备
- 流数据读写
- 大数据集
- 简单的文件模型
- 强大的跨平台兼容性

HDFS特殊的设计，在实现上述优良特性的同时，也使得自身具有一些应用局限性，主要包括以下几个方面：

- 不适合低延迟数据访问
- 无法高效存储大量小文件
- 不支持多用户写入及任意修改文件

## 2.1.4块

HDFS默认一个块64MB（或者128M），一个文件被分成多个块，以块作为存储单位
块的大小远远大于普通文件系统，可以最小化寻址开销
HDFS采用抽象的块概念可以带来以下几个明显的好处：

- **支持大规模文件存储**：文件以块为单位进行存储，一个大规模文件可以被分拆成若干个文件块，不同的文件块可以被分发到不同的节点上，因此，一个文件的大小不会受到单个节点的存储容量的限制，可以远远大于网络中任意节点的存储容量
-  **简化系统设计**：首先，大大简化了存储管理，因为文件块大小是固定的，这样就可以很容易计算出一个节点可以存储多少文件块；其次，方便了元数据的管理，元数据不需要和文件块一起存储，可以由其他系统负责管理元数据
- **适合数据备份**：每个文件块都可以冗余存储到多个节点上，大大提高了系统的容错性和可用性
### 2.1.5NameNode&DataNode
|NameNode|DataNode|
|--|--|
|存储元数据|存储文件内容|
|元数据保存在内存中|文件内容保存在磁盘|
|保存文件，block，datanode之间的映射关系|维护了block id 到datanode本地文件的映射关系|
### NameNode
- 负责管理分布式文件系统的命名空间（Namespace），保存了两个核心的数据结构，即FsImage（维护文件系统树以及文件树中所有的文件和文件夹的元数据）和EditLog（记录了所有针对文件的创建、删除、重命名等操作）
- 名称节点记录了每个文件中各个块所在的数据节点的位置信息
![d06b0ef38664bdf416b5c6ef80b88ae3](https://github.com/HDZ12/Big-Data-System/assets/99587726/483fa465-1883-4968-838d-537b27483746)
### FsImage

- FsImage文件包含文件系统中所有目录和文件inode的序列化形式。每个inode是一个文件或目录的元数据的内部表示，并包含此类信息：文件的复制等级、修改和访问时间、访问权限、块大小以及组成文件的块。对于目录，则存储修改时间、权限和配额元数据
- FsImage文件没有记录文件包含的每个块存储在哪个数据节点。而是由名称节点把这些映射信息保留在内存中，当数据节点加入HDFS集群时，数据节点会把自己所包含的块列表告知给名称节点，此后会定期执行这种告知操作，以确保名称节点的块映射是最新的。
- 因为考虑到元数据信息可能随着数据的变多而不断变大，为了缩小文件的空间大小，需要存储为二进制文件，因此查看时需要命令hdfs oiv进行转换
![8e16a4108e6209084a90cf0b9a3e30a3](https://github.com/HDZ12/Big-Data-System/assets/99587726/68444405-11c5-4417-8991-5b402f211b65)
- imgVersion(int)：当前image的版本信息
-  namespaceID(int)：用来确保别的HDFS instance中的datanode不会误连上当前NN。
-  numFiles(long)：整个文件系统中包含有多少文件和目录
- genStamp(long)：生成该image时的时间戳信息。
- path(String)：该目录的路径，如”/user/build/build-index”
-  replications(short)：副本数(目录虽然没有副本，但这里记录的目录副本数也为3)
-  mtime(long)：该目录的修改时间的时间戳信息
- atime(long)：该目录的访问时间的时间戳信息
- blocksize(long)：目录的blocksize都为0
- numBlocks(int)：实际有多少个文件块，目录的该值都为-1，表示该item为目录
- nsQuota(long)：namespace Quota值，限制目录下的文件数量，若没加Quota限制则为-1
- dsQuota(long)：disk Quota值，若没加限制则也为-1
-  username(String)：该目录的所属用户名
- group(String)：该目录的所属组
-  permission(short)：该目录的permission信息，如644等，有一个short来记录。
如果读到的是文件还有如下信息：
- blockid(long)：属于该文件的block的blockid，
- numBytes(long)：该block的大小
- genStamp(long)：该block的时间戳
### Editlog

- 在名称节点启动的时候，它会将FsImage文件中的内容加载到内存中，之后再执行EditLog文件中的各项操作，使得内存中的元数据和实际的同步，存在内存中的元数据支持客户端的读操作
- 一旦在内存中成功建立文件系统元数据的映射，则创建一个新的FsImage文件和一个空的EditLog文件
- 名称节点起来之后，HDFS中的更新操作会重新写到EditLog文件中，因为FsImage文件一般都很大（GB级别的很常见），如果所有的更新操作都往FsImage文件中添加，这样会导致系统运行的十分缓慢，但是，如果往EditLog文件里面写就不会这样，因为EditLog 要小很多。每次执行写操作之后，且在向客户端发送成功代码之前，edits文件都需要同步更新
**EditLog不断变大问题**:

在名称节点运行期间，HDFS的所有更新操作都是直接写到EditLog中，久而久之， EditLog文件将会变得很大虽然这对名称节点运行时候是没有什么明显影响的，但是，当名称节点重启的时候，名称节点需要先将FsImage里面的所有内容映像到内存中，然后再一条一条地执行EditLog中的记录，当EditLog文件非常大的时候，会导致名称节点启动操作非常慢，而在这段时间内HDFS系统处于安全模式，一直无法对外提供写操作，影响了用户的使用
***如何解决？答案是：SecondaryNameNode第二名称节点***

第二名称节点是HDFS架构中的一个组成部分，它是用来保存名称节点中对HDFS 元数据信息的备份，并减少名称节点重启的时间。SecondaryNameNode一般是单独运行在一台机器上
==SecondaryNameNode工作情况==：

1. SecondaryNameNode会定期和NameNode通信，请求其停止使用EditLog文件，暂时将新的写操作写到一个新的文件edit.new上来，这个操作是瞬间完成，上层写日志的函数完全感觉不到差别；
2. SecondaryNameNode通过HTTP GET方式从NameNode上获取到FsImage和EditLog文件，并下载到本地的相应目录下；
3. SecondaryNameNode将下载下来的FsImage载入到内存，然后一条一条地执行EditLog文件中的各项更新操作，使得内存中的FsImage保持最新；这个过程就是EditLog和FsImage文件合并；
4. SecondaryNameNode执行完（3）操作之后，会通过post方式将新的FsImage文件发送到NameNode节点上
5. NameNode将从SecondaryNameNode接收到的新的FsImage替换旧的FsImage文件，同时将edit.new替换EditLog文件，通过这个过程EditLog就变小了
![01525ddbb60e2a087a084ca038f0b558](https://github.com/HDZ12/Big-Data-System/assets/99587726/69343cea-8e38-4daa-aaf8-988854eb1f73)
### DataNode

- 数据节点是分布式文件系统HDFS的工作节点，负责数据的存储和读取，会根据客户端或者是名称节点的调度来进行数据的存储和检索，并且向名称节点定期发送自己所存储的块的列表
- 每个数据节点中的数据会被保存在各自节点的本地Linux文件系统中

## 2.1.5HDFS体系结构

 HDFS采用了主从（Master/Slave）结构模型，一个HDFS集群包括一个名称节点（NameNode）和若干个数据节点（DataNode）（如图3-4所示）。名称节点作为中心服务器，负责管理文件系统的命名空间及客户端对文件的访问。集群中的数据节点一般是一个节点运行一个数据节点进程，负责处理文件系统客户端的读/写请求，在名称节点的统一调度下进行数据块的创建、删除和复制等操作。每个数据节点的数据实际上是保存在本地Linux文件系统中的

## 2.1.6HDFS命名空间管理

- HDFS的命名空间包含目录、文件和块
- 在HDFS1.0体系结构中，在整个HDFS集群中只有一个命名空间，并且只有唯一一个名称节点，该节点负责对这个命名空间进行管理
- HDFS使用的是传统的分级文件体系，因此，用户可以像使用普通文件系统一样，创建、删除目录和文件，在目录间转移文件，重命名文件等

## 2.1.7通信协议

- HDFS是一个部署在集群上的分布式文件系统，因此，很多数据需要通过网络进行传输
- 所有的HDFS通信协议都是构建在TCP/IP协议基础之上的
- 客户端通过一个可配置的端口向名称节点主动发起TCP连接，并使用客户端协议与名称节点进行交互
- 名称节点和数据节点之间则使用数据节点协议进行交互
- 客户端与数据节点的交互是通过RPC（Remote Procedure Call）来实现的。在设计上，名称节点不会主动发起RPC，而是响应来自客户端和数据节点的RPC请求

## 2.1.8HDFS体系结构的局限性

HDFS只设置唯一一个名称节点，这样做虽然大大简化了系统设计，但也带来了一些明显的局限性，具体如下：

1. 命名空间的限制：名称节点是保存在内存中的，因此，名称节点能够容纳的对象（文件、块）的个数会受到内存空间大小的限制。
2. 性能的瓶颈：整个分布式文件系统的吞吐量，受限于单个名称节点的吞吐量。
3. 隔离问题：由于集群中只有一个名称节点，只有一个命名空间，因此，无法对不同应用程序进行隔离。
4. 集群的可用性：一旦这个唯一的名称节点发生故障，会导致整个集群变得不可用

## 2.1.9HDFS存储原理

### 冗余数据保存

  作为一个分布式文件系统，为了保证系统的容错性和可用性，HDFS采用了多副本方式对数据进行冗余存储，通常一个数据块的多个副本会被分布到不同的数据节点上，如图所示，数据块1被分别存放到数据节点A和C上，数据块2被存放在数据节点A和B上。这种多副本方式具有以下几个优点：

- 加快数据传输速度
- 容易检查数据错误
- 保证数据可靠性

### 数据存放

- 第一个副本：放置在上传文件的数据节点；如果是集群外提交，则随机挑选一台磁盘不太满、CPU不太忙的节点
- 第二个副本：放置在与第一个副本不同的机架的节点上
- 第三个副本：与第一个副本相同机架的其他节点上
- 更多副本：随机节点

### 距离

距离：要计算其中的距离，我们首先要了解HDFS中是如何定义节点间的距离的，其中涉及拓扑逻辑的概念

每个节点间的距离计算方式是通过寻找最近公共祖先所需要的距离作为最终的结果
![93a5524ab225c1335030b1c117db2956](https://github.com/HDZ12/Big-Data-System/assets/99587726/9504554e-96fd-4b12-a6f4-f5e6c2e5e540)
### 数据存放

在现有的 HDFS 中，我们可以对块的网络拓朴位置进行策略的选择，同样， 对于数据的存储介质， HDFS 有对应的若干种策略

- HOT
- COLD
- WARM
- ALL_SSD
- ONE_SSD
- LAZY_PERSIST

在这6种策略中，前3种策略和后3种策略可以看作是两大类，前3 种策略是根据冷热数据的角度来区分的，后3种策略是根据存放盘的性质来区分的
### NameNode选择Datanode时应考虑的因素

- 存储不能是READONLY （只读）。
- 存储不能是坏的。
- 存储所在节点不应该是已下线或下线中的节点。
- 存储所在节点不应该是消息落后的节点，实际指的是一段时间内没有更新心跳的节点。

- 节点内保证有足够的剩余空间能满足写块所要求的大小。
- 要考虑节点的IO负载繁忙程度。
- 要满足同机架内最大副本数的限制。

### 心跳机制

 HDFS是主从架构，所以为了实时的得知dataNode是否存活，必须建立心跳机制，在整个hdfs运行过程中，dataNode会定时的向nameNode发送心跳报告已告知nameNode自己的状态。

**心跳内容：**

- 报告自己的存活状态，每次汇报之后都会更新维护的计数信息  
- 向nameNode汇报自己的存储的block列表信息

**nameNode判断一个dataNode宕机的基准：**连续10次接收不到dataNode的心跳信息，和2次的检查时间。

## 文件操作

### 写数据
![3617c4943cdc6c5ffb84e595c4a4ede1](https://github.com/HDZ12/Big-Data-System/assets/99587726/6065ec18-a015-432b-9ad1-84c2addccfa8)
### block,packet,chunk

-  block是最大的一个单位，它是最终存储于DataNode上的数据粒度，由dfs.block.size参数决定，默认是64M；注：这个参数由客户端配置决定；
- packet是中等的一个单位，它是数据由Client流向DataNode的粒度，以dfs.write.packet.size参数为参考值，默认是64K；注：这个参数为参考值，是指真正在进行数据传输时，会以它为基准进行调整，调整的原因是一个packet有特定的结构，调整的目标是这个packet的大小刚好包含结构中的所有成员，同时也保证写到DataNode后当前block的大小不超过设定值；
-  chunk是最小的一个单位，它是Client到DataNode数据传输中进行数据校验的粒度，由io.bytes.per.checksum参数决定，默认是512B；注：事实上一个chunk还包含4B的校验值，因而chunk写入packet时是516B；数据与检验值的比值为128:1，所以对于一个64M的block会有一个0.5M的校验文件与之对应；

一个Packet是由Header和Data两部分组成，其中Header部分包含了一个Packet的概要属性信息，如下表所示：
|字段名称|字段类型|字段长度|字段含义|
|--|--|--|--|
|PktLen|int|4|Packet的长度|
|offsetInBlock|long|8|Packet在Block中的偏移量|
|SeqNo|long|8|Packet序列号，在同一Block中唯一|
|LastPacketInBlock|Boolean|1|是否是一个Block的最后一个packet|
|DataLen|int|4|不包含header和checksum的长度|
写入文件：写过程中会以chunk、packet及packet queue三个粒度做三层缓存；

- 首先，当数据流入DFSOutputStream时，DFSOutputStream内会有一个chunk大小的buf，当数据写满这个buf（或遇到强制flush），会计算checksum值，然后填塞进packet；
- 当一个chunk填塞进入packet后，仍然不会立即发送，而是累积到一个packet填满后，将这个packet放入dataqueue队列；
- 进入dataqueue队列的packet会被另一线程按序取出发送到第一个datanode，并将该Packet从dataQueue队列中移到ackQueue队列中；
- Packet由第一个datanode接收并存储之后，再经由pipeline 传递给下一个 datanode，直到最后一个 datanode；
- 最后一个datanode成功存储之后会向上游发送确认信息，DFSOutputStream收到确认信息后，将该packet从ackQueue队列删除。
![3617c4943cdc6c5ffb84e595c4a4ede1](https://github.com/HDZ12/Big-Data-System/assets/99587726/4b5db0c6-d26d-41da-aaf5-08293f14fc2a)
### 读数据
![57b8f19a0f9fcf2c074d78a3168e16c0](https://github.com/HDZ12/Big-Data-System/assets/99587726/17143b39-9267-4024-8909-85fbfda9ffbc)
### 删除文件

- 客户端调用HDFS的FileSystem实例，也就是DistributedFileSystem的delete()方法向NameNode发送数据“删除”操作。
- DistributedFileSystem对象通过RPC调用NameNode节点上的delete()，它只标记操作要涉及的需要被删除的数据块、并将delete操作信息持久化到编辑到Edit Log，而不会主动联系保存这些数据块的数据节点，立即删除数据。
- NameNode更新完给客服返回一个ack信号，表示删除成功
- 当保存着这些数据块的DataNode节点向NameNode节点发送“心跳”，向DataNode节点报告自己当前的相关信息时，在NameNode给DataNode的心跳应答里，NameNode节点会通过DatanodeCommand命令数据节点删除数据。
- 在这个过程中，读者需要注意两个要点：被删除文件的数据，也就是该文件对应的数据块，在删除操作完成后的一段时间以后，才会被真正删除;名字节点和数据节点间永远维持着简单的主从关系，名字节点不会向数据节点发起任何IPC调用，数据节点需要配合名字节点执行操作，都是通过数据节点发送心跳应答中的DatanodeCommand返回。


